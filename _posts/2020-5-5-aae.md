---
layout: post
title:  "Adversarial Autoencoder"
author: "Ke He"
description: "How Adversarial autoencoders can be used in traffic analysis"
---

## Adversarial Autoencoders

Adversarial autoencoders are a special type of autoencoder, where a discriminator is placed that takes the latent space as input and classifies it as either real or fake based on prior distributions. In some ways it is similar to Variational Autoencoder, both models tries to place some constraints into the latent space, but the downside of autoencoder is that it assumes Gaussian distribution in the latent space, and is hard to train properly. Adversarial Autoencoders can be trained with any prior distribution.

## Types of AAE

The encoder of aae can have various possible structure:
- deterministic: in this case the encoder outputs a single value, which is similar to standard autoencoder
- gaussian posterior: in this case the encoder function assumes a gaussian distribution, similar to variational autoencoders
- Universal approximator posterior: the encoder function posterior can assume any distribution
The authors found using the three type of encoders have similar results

## Comparison to VAE
The authors in the paper states for MNIST samples, aae have sharp transitions in its latent space and no holes, whereas vae have various gap inbetween clusters. This implies aae have learnt a better manifold.

### Incorporating labels

Labels can be provided to the discriminator along with the sample distribution, so that we can have a finer control over the distribution of each class. Note that an extra class is provided for unlabelled data, where the distribution would assume the overall distribution of the labelled class.

A class label can also be provided to the decoder to obtain extra information about the decoding process. For example, an aae trained with MNIST dataset may use latent space to learn the style, and a label to generate the digit based on style
