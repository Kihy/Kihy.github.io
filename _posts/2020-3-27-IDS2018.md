---
layout: post
title:  "IDS2018 Dataset"
author: "Ke He"
description: "Using IDS 2018 dataset"
---
###IDS2018
The IDS2018 dataset consists is located on amazon web server, which is around 220G in total. Fortunately it consists of raw traffic data and extracted data. The extracted data is only ~6G and can be downloaded overnight. Again the extracted data contains some columns that are not wanted, thus we have to map the columns again.

The files are quite large, the largesting being 4G, thus to prevent it using all of the computer's memory the chunsize argument in pd.read_csv is used. A problem with the files is that after 100000 lines, the headers appear again in the same file for some of the files. So we have to sanitize the files.

The files were sanitized with sed commands to remove lines containing the headers. Then it was found some fields have '' as its value, thus check_float has been changed to set '' as 0

The files are too large to do all calculations at once, thus currently we are only gathering files for a specific attack. In the future it is possible to consider processing chunks at a time, split them, and have a map that includes new label values.

When training it also complains that 979781000000 is not a valid int32, so we have to use the dtypes field in metadata to define column_defaults so that it defaults to int64. Doing so will break the labels, which means we have to manually change labels type to uint8.

###Results
The model trained with benign and Patator samples fails to identify our attack.
