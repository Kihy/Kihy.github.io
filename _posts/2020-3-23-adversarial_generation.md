---
layout: post
title:  "Adversarial Generation"
author: "Ke He"
description: "Generation of Adversarial examples with JSMA"
---

### Dataset Input change
Since we are loading batched sample with make_csv_dataset, we have to change the way we are finding samples to perturbate. The input batch has batch size 1, as iterative methods such as JSMA is very hard to do in batches since different samples stops at different iterations. Then the sample is unbatched so that .filter() can be used (Alternative solution is to use .map() after batch but it is slow) to filter out all samples that is already target class. Then it is batched again with batch_size 1.

### visualizing perturbated data
The visualizating is slightly more complicated than before. Since the scaling is done in the model, we have to extract the scaling layer and pass input through for scaled input.

### Findings
A simple experiment was ran, The dataset involves 3 categories: BENIGN, Dos Hulk and PortScan, and 1000 samples from the validation set was to be used for perturbation. The results show that the adversarial examples for the two attacks can be generated by simply changing ACK, FIN, Fwd PSH, etc, flag counts. Since the data range for the flags are 0~1, the algorithm has to change it by 1, which is a large change (quivalent of changing the duration field from 0 to 65535) and could cause the classifier to change its output. The next step is to see if it is actually possible to change the flag count of the attack without affecting the functionality of the attack.
