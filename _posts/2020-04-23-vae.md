---
layout: post
title:  "Variational Autoencoders"
author: "Ke He"
description: "Using Variational Autoencoders to visualize our dataset"
---

## VAE

VAE is a generative model. Unlike vanilla autoencoders, it has an additional kullback leibler divergence term in its loss function. The idea behind it is that in vanilla autoencoder, the latent space has basically no structure at all. This means gaps between sample points in the latent space often generates useless information with the decoder. In the VAE case, the latent space is optimised to have similar distributions as standard normal distribution with the kullback leibler divergence term. This adds structure in the latent space, and the gaps between two points will be meaningful.

## Implementation Details
In practice VAE is hard to optimize. The result of using stardard binary_crossentropy reconstruction error and kl divergence shows:

![Standard reconstruction error](../assets/images/charts/standard_recon.png)

This was due to reconstruction error being too small compared to kl divergence, thus the optimizer is only reducing kl divergence, which makes both classes have similar distribution.

We can also visualize the dimensions of a particular sample as follows:
![Latent dimension of Standard reconstruction error](../assets/images/charts/latent_dim1.png)
And all three dimension have exactly the same distribution, which is not useful.

In order to make the model focus on reconstruction error more, the reconstruction error is multiplied by a particular number. For a value of 48, which is the input dimension, the clustering performance is increased significantly:
![weighted reconstruction error](../assets/images/charts/weighted_recon.png)

and the corresponding latent dimensions:
![Latent dimension of weighted reconstruction error](../assets/images/charts/latent_dim48.png)
